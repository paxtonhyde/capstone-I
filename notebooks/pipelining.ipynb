{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as sparktypes\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (ps.sql.SparkSession.builder \n",
    "    .master(\"local\") \n",
    "    .appName(\"pipeline\")\n",
    "    .getOrCreate()\n",
    "    )\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading responses with survey data into a spark dataframe\n",
    "path = \"../data/SharedResponsesSurvey_10000.csv\"\n",
    "responses = spark.read.csv(path, header=True)\n",
    "\n",
    "## pullin out all the countries (n < 100)\n",
    "path = \"../data/country_cluster_map.csv\"\n",
    "countries = spark.read.csv(path, header=True).select(\"ISO3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_intervention(dataf):\n",
    "    '''\n",
    "    Returns the proportion of choices in dataf that favored\n",
    "    intervention over non-intervention, and n the number of choices analyzed.\n",
    "        Params: dataf (Spark Dataframe)\n",
    "        Returns: p (float), n (int)\n",
    "    '''\n",
    "    # probability of having chosen commission\n",
    "    commits = dataf.filter(\"Saved = 1 AND Intervention = 1\").count()\n",
    "    # probability of having not chosen omission, meaning that the user must have chosen\n",
    "    # commission in the scenario\n",
    "    omits = dataf.filter(\"Saved = 0 AND Intervention = 0\").count()\n",
    "    n = dataf.count()\n",
    "    try:\n",
    "        return (round((commits + omits) / n, 4), n)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"p_intervention received a dataframe without revelant entries.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_legality(dataf):\n",
    "    '''\n",
    "    Returns p the proportion of choices from data in dataf that favored saving pedestrians\n",
    "    crossing legally, and n the number of choices with the legal dimension.\n",
    "        Params: dataf (Spark Dataframe)\n",
    "        Returns: tuple: (p (float), n (int))\n",
    "    '''\n",
    "    legality = dataf.filter(\"CrossingSignal != 0 AND PedPed = 1\")\n",
    "    ## above line credit Edmond Awad, MMFunctionsShared.R\n",
    "    ## found at: https://osf.io/3hvt2/files/\n",
    "    n = legality.count()\n",
    "    \n",
    "    # probability of having chosen to save law-abiding\n",
    "    peds = legality.filter(\"Saved = 1 AND CrossingSignal = 1\").count()\n",
    "    # probability of having chosen to not save non-law-abiding\n",
    "    jwalkers = legality.filter(\"Saved = 0 AND CrossingSignal = 2\").count()\n",
    "    \n",
    "    try:\n",
    "        return (round((peds + jwalkers) / n, 4), n)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"p_legality received a dataframe without revelant entries.\")\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_factor(dataf, attribute):\n",
    "    '''\n",
    "       Returns the proportion of choices from data in dataf that favored the default choice, \n",
    "    default (str) the default choice for the factor, nondefault (str) the alternative choice\n",
    "    for the factor, and n (int) the number of choices analyzed with the factor corresponding\n",
    "    to the dimension.\n",
    "        Parameters: dataf (Spark Dataframe), attribute (str)\n",
    "        Returns: tuple: p (float), n (int), default (str), nondefault (str),\n",
    "    '''\n",
    "    attr = {\"Utilitarian\" : ['More', 'Less']\\\n",
    "              , \"Gender\" : ['Male', 'Female']\\\n",
    "              , \"Social Status\" : ['High', 'Low']\\\n",
    "              , \"Age\" : ['Young', 'Old']\\\n",
    "             , \"Species\" : []\\\n",
    "             , \"Fitness\" : []}\n",
    "    try:\n",
    "        default, nondefault = attr[attribute]\n",
    "    except KeyError:\n",
    "        print(\"p_factor received an invalid attribute.\")\n",
    "        return None  \n",
    "    \n",
    "    factor = dataf.filter(f\"ScenarioType = '{attribute}' \")\n",
    "    n = factor.count()\n",
    "    # probability of having chosen the default\n",
    "    defs = factor.filter(f\"Saved = 1 AND AttributeLevel = '{default}'\").count()\n",
    "    # probability of having not chosen the nondefault\n",
    "    nonnondefs = factor.filter(f\"Saved = 0 AND AttributeLevel = '{nondefault}'\").count()\n",
    "    try:\n",
    "        return ( round((defs + nonnondefs) / n, 4), n, default, nondefault )\n",
    "    except ZeroDivisionError:\n",
    "        print(\"p_factor received a dataframe without revelant entries.\")\n",
    "        return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(UserCountry3='POL'),\n",
       " Row(UserCountry3='LVA'),\n",
       " Row(UserCountry3='BRA'),\n",
       " Row(UserCountry3='FRA'),\n",
       " Row(UserCountry3='ITA')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_probs = responses.select(\"UserCountry3\").groupby(\"UserCountry3\").count()\n",
    "country_probs.select(\"UserCountry3\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_probs = responses.select(\"UserCountry3\").groupby(\"UserCountry3\")\n",
    "\n",
    "int_p = udf(lambda frame: p_intervention(frame)[0], sparktypes.FloatType())\n",
    "country_probs = country_probs.apply(int_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: DataFrame[ResponseID: string, ExtendedSessionID: string, UserID: string, ScenarioOrder: string, Intervention: string, PedPed: string, Barrier: string, CrossingSignal: string, AttributeLevel: string, ScenarioTypeStrict: string, ScenarioType: string, DefaultChoice: string, NonDefaultChoice: string, DefaultChoiceIsOmission: string, NumberOfCharacters: string, DiffNumberOFCharacters: string, Saved: string, Template: string, DescriptionShown: string, LeftHand: string, UserCountry3: string, Review_age: string, Review_education: string, Review_gender: string, Review_income: string, Review_political: string, Review_religious: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-a8cb57c0e1d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mint_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp_intervention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparktypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mint_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp_intervention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparktypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcountry_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountry_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p_intervention'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_responses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcountry_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountry_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_intervention'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_responses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# This function is for improving the online help system in the interactive interpreter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: DataFrame[ResponseID: string, ExtendedSessionID: string, UserID: string, ScenarioOrder: string, Intervention: string, PedPed: string, Barrier: string, CrossingSignal: string, AttributeLevel: string, ScenarioTypeStrict: string, ScenarioType: string, DefaultChoice: string, NonDefaultChoice: string, DefaultChoiceIsOmission: string, NumberOfCharacters: string, DiffNumberOFCharacters: string, Saved: string, Template: string, DescriptionShown: string, LeftHand: string, UserCountry3: string, Review_age: string, Review_education: string, Review_gender: string, Review_income: string, Review_political: string, Review_religious: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "## wanting to do it all in spark ... might be too hard\n",
    "pandas_cols = [\"ISO3\", \"p_intervention\", \"n_intervention\", \"p_legality\", \"n_legality\",\\\n",
    "               \"p_util\", \"n_util\", \"p_gender\", \"n_gender\", \\\n",
    "               \"p_social\", \"n_social\", \"p_age\", \"n_age\"]\n",
    "country_probs = responses.select(\"UserCountry3\").groupby(\"UserCountry3\").count()\n",
    "#country_probs = pd.DataFrame(columns=pandas_cols)\n",
    "\n",
    "country_probs = responses.select('*').groupby(\"UserCountry3\").agg({})\n",
    "\n",
    "for row in country_probs.select(\"UserCountry3\").take(2):\n",
    "    country = row.UserCountry3\n",
    "    country_responses = responses.filter(f\"UserCountry3 = '{country}' \")\n",
    "    \n",
    "    factors = [\"Utilitarian\", \"Gender\", \"Social Status\", \"Age\"]\n",
    "    \n",
    "    #intervention\n",
    "    int_p = udf(lambda frame: p_intervention(frame)[0], sparktypes.FloatType())\n",
    "    int_n = udf(lambda frame: p_intervention(frame)[1], sparktypes.IntegerType())\n",
    "    country_probs = country_probs.withColumn('p_intervention', int_p(country_responses))\n",
    "    country_probs = country_probs.withColumn('n_intervention', int_n(country_responses))\n",
    "#     try:\n",
    "#         #legality\n",
    "#         #country_data_out.extend(p_legality(country_responses))\n",
    "        \n",
    "# #         for fac in factors:\n",
    "# #             p, n, deflt, nondeflt = p_factor(country_responses, fac)\n",
    "# #             country_data_out.extend((p, n))\n",
    "#     except TypeError:\n",
    "#         print(f\"{country} had no relevant entries.\")\n",
    "#         continue\n",
    "    \n",
    "    country_probs.show()\n",
    "#     country_data_out_df = pd.DataFrame([country_data_out], columns=pandas_cols)\n",
    "#     country_probs = country_probs.append(country_data_out_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58 entries, 0 to 57\n",
      "Data columns (total 8 columns):\n",
      "Unnamed: 0          58 non-null int64\n",
      "ISO3                58 non-null object\n",
      "p_n_intervention    58 non-null object\n",
      "p_n_legality        56 non-null object\n",
      "p_n_util            58 non-null object\n",
      "p_n_gender          58 non-null object\n",
      "p_n_social          58 non-null object\n",
      "p_n_age             58 non-null object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.8+ KB\n",
      "('(', '0', '.', '3', '8', '8', '9', ',', ' ', '3', '6', ')')\n"
     ]
    }
   ],
   "source": [
    "country_pref_types = {\"ISO3\" : \"str\", \"p_intervention\" : \"float64\", \"n_intervention\" : \"int64\",\\\n",
    "                    \"p_legality\" : \"float64\", \"n_legality\" : \"int64\",\\\n",
    "                    \"p_util\" : \"float64\", \"n_util\" : \"int64\",\\\n",
    "                    \"p_gender\" : \"float64\", \"n_gender\" : \"int64\", \\\n",
    "                   \"p_social\" : \"float64\", \"n_social\" : \"int64\",\\\n",
    "                      \"p_age\" : \"float64\", \"n_age\" : \"int64\"}\n",
    "\n",
    "data = pd.read_csv(\"../data/country_preferences.csv\", dtype=country_pref_types)\n",
    "data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
